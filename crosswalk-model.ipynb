{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosswalk Model\n",
    "Below we train a CNN model to detect crosswalks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from PIL import Image \n",
    "from PIL.ImageDraw import Draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import ##\n",
    "Let's import our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cvs = './data/train/_annotations.csv'\n",
    "train_images = './data/train/'\n",
    "\n",
    "test_cvs = './data/test/_annotations.csv'\n",
    "test_images = './data/test/'\n",
    "\n",
    "valid_cvs = './data/valid/_annotations.csv'\n",
    "valid_images = './data/valid/'\n",
    "\n",
    "classes = ['crosswalk']\n",
    "\n",
    "def load_data(csv_file, image_dir):\n",
    "    training_image_records = pd.read_csv(csv_file)\n",
    "    train_image_path = os.path.join(os.getcwd(), image_dir)\n",
    "\n",
    "    train_images = []\n",
    "    train_targets = []\n",
    "    train_labels = []\n",
    "\n",
    "    for index, row in training_image_records.iterrows():\n",
    "        \n",
    "        (filename, width, height, class_name, xmin, ymin, xmax, ymax) = row\n",
    "        \n",
    "        train_image_fullpath = os.path.join(image_dir, filename)\n",
    "        train_img = keras.preprocessing.image.load_img(train_image_fullpath, target_size=(height, width))\n",
    "        train_img_arr = keras.preprocessing.image.img_to_array(train_img)\n",
    "        \n",
    "        train_images.append(train_img_arr)\n",
    "        train_targets.append((xmin, ymin, xmax, ymax))\n",
    "        train_labels.append(classes.index(class_name))\n",
    "\n",
    "    return np.array(train_labels), np.array(train_targets), np.array(train_images)\n",
    "\n",
    "train_labels, train_targets, train_images = load_data(train_cvs, train_images)\n",
    "test_labels, test_targets, test_images = load_data(test_cvs, test_images)\n",
    "valid_labels, valid_targets, valid_images = load_data(valid_cvs, valid_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Model ##\n",
    "We create a CNN model which has the ability to show bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 640, 640\n",
    "input_shape = (height, width, 3)\n",
    "input_layer = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "#create the base layers\n",
    "base_layers = layers.experimental.preprocessing.Rescaling(1./255, name='bl_1')(input_layer)\n",
    "base_layers = layers.Conv2D(16, 3, padding='same', activation='relu', name='bl_2')(base_layers)\n",
    "base_layers = layers.MaxPooling2D(name='bl_3')(base_layers)\n",
    "base_layers = layers.Conv2D(32, 3, padding='same', activation='relu', name='bl_4')(base_layers)\n",
    "base_layers = layers.MaxPooling2D(name='bl_5')(base_layers)\n",
    "base_layers = layers.Conv2D(64, 3, padding='same', activation='relu', name='bl_6')(base_layers)\n",
    "base_layers = layers.MaxPooling2D(name='bl_7')(base_layers)\n",
    "base_layers = layers.Flatten(name='bl_8')(base_layers)\n",
    "\n",
    "#create the classifier branch\n",
    "classifier_branch = layers.Dense(128, activation='relu', name='cl_1')(base_layers)\n",
    "classifier_branch = layers.Dense(len(classes), name='cl_head')(classifier_branch) \n",
    "\n",
    "#create the localiser branch\n",
    "locator_branch = layers.Dense(128, activation='relu', name='bb_1')(base_layers)\n",
    "locator_branch = layers.Dense(64, activation='relu', name='bb_2')(locator_branch)\n",
    "locator_branch = layers.Dense(32, activation='relu', name='bb_3')(locator_branch)\n",
    "locator_branch = layers.Dense(4, activation='sigmoid', name='bb_head')(locator_branch)\n",
    "\n",
    "model = tf.keras.Model(input_layer, outputs=[classifier_branch, locator_branch])\n",
    "losses = {\"cl_head\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \"bb_head\": tf.keras.losses.MSE}\n",
    "model.compile(loss=losses, optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 31s 750ms/step - loss: 164698.7969 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164698.7969 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.1384 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 32s 793ms/step - loss: 164677.8594 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164677.8594 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 34s 862ms/step - loss: 164677.8594 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164677.8594 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 33s 813ms/step - loss: 164677.8594 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164677.8594 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 32s 791ms/step - loss: 164677.8750 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164677.8750 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 31s 770ms/step - loss: 164677.8594 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164677.8594 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 31s 783ms/step - loss: 164677.8594 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164677.8594 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 32s 799ms/step - loss: 164677.8750 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164677.8750 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 33s 820ms/step - loss: 164677.8594 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164677.8594 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 33s 825ms/step - loss: 164677.9062 - cl_head_loss: 0.0000e+00 - bb_head_loss: 164677.9062 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00 - val_loss: 171771.5469 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 171771.5469 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "trainTargets = {\n",
    "    \"cl_head\": train_labels,\n",
    "    \"bb_head\": train_targets\n",
    "}\n",
    "validTargets = {\n",
    "    \"cl_head\": valid_labels,\n",
    "    \"bb_head\": valid_targets\n",
    "}\n",
    "\n",
    "training_epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "            train_images, trainTargets,\n",
    "            validation_data=(valid_images, validTargets),\n",
    "            batch_size=4,\n",
    "            epochs=training_epochs,\n",
    "            shuffle=True,\n",
    "            verbose=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [164698.796875,\n",
       "  164677.859375,\n",
       "  164677.859375,\n",
       "  164677.859375,\n",
       "  164677.875,\n",
       "  164677.859375,\n",
       "  164677.859375,\n",
       "  164677.875,\n",
       "  164677.859375,\n",
       "  164677.90625],\n",
       " 'cl_head_loss': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'bb_head_loss': [164698.796875,\n",
       "  164677.859375,\n",
       "  164677.859375,\n",
       "  164677.859375,\n",
       "  164677.875,\n",
       "  164677.859375,\n",
       "  164677.859375,\n",
       "  164677.875,\n",
       "  164677.859375,\n",
       "  164677.90625],\n",
       " 'cl_head_accuracy': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'bb_head_accuracy': [0.138364776968956,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'val_loss': [171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875],\n",
       " 'val_cl_head_loss': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'val_bb_head_loss': [171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875,\n",
       "  171771.546875],\n",
       " 'val_cl_head_accuracy': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'val_bb_head_accuracy': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 152123.5000 - cl_head_loss: 0.0000e+00 - bb_head_loss: 152123.5000 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0000e+00\n",
      "test loss, test acc: [152123.5, 0.0, 152123.5, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "testTargets = {\n",
    "    \"cl_head\": test_labels,\n",
    "    \"bb_head\": test_targets\n",
    "}\n",
    "\n",
    "results = model.evaluate(test_images, testTargets, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 701ms/step\n",
      "[[-0.10527883]\n",
      " [-0.09087132]\n",
      " [-0.09087132]\n",
      " [-0.10897115]\n",
      " [-0.09434853]\n",
      " [-0.04892787]\n",
      " [-0.11188803]\n",
      " [-0.11188803]\n",
      " [-0.1216284 ]\n",
      " [-0.0946286 ]\n",
      " [-0.0946286 ]\n",
      " [-0.10151491]\n",
      " [-0.06746612]\n",
      " [-0.06746612]\n",
      " [-0.11542323]\n",
      " [-0.10449751]\n",
      " [-0.11054926]\n",
      " [-0.13268134]\n",
      " [-0.12694566]\n",
      " [-0.02925179]\n",
      " [-0.07820338]\n",
      " [-0.10469791]\n",
      " [-0.11056319]\n",
      " [-0.09298025]\n",
      " [-0.09298035]\n",
      " [-0.04069618]\n",
      " [-0.04069618]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_images)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(preds)\n\u001b[0;32m----> 4\u001b[0m (startX, startY, endX, endY) \u001b[38;5;241m=\u001b[39m preds\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# load the input image (in OpenCV format), resize it such that it\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# fits on our screen, and grab its dimensions\u001b[39;00m\n\u001b[1;32m      7\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(imagePath)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# make bounding box predictions on the input image\n",
    "preds = model.predict(test_images)[0]\n",
    "print(preds)\n",
    "(startX, startY, endX, endY) = preds\n",
    "# load the input image (in OpenCV format), resize it such that it\n",
    "# fits on our screen, and grab its dimensions\n",
    "image = cv2.imread(imagePath)\n",
    "image = imutils.resize(image, width=600)\n",
    "(h, w) = image.shape[:2]\n",
    "# scale the predicted bounding box coordinates based on the image\n",
    "# dimensions\n",
    "startX = int(startX * w)\n",
    "startY = int(startY * h)\n",
    "endX = int(endX * w)\n",
    "endY = int(endY * h)\n",
    "# draw the predicted bounding box on the image\n",
    "cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "    (0, 255, 0), 2)\n",
    "# show the output image\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
